---
title: "Complete GeneralConverter with Parallel Processing and Robust Data Handling"
date_created: 2026-02-02
category: performance-issues
tags:
  - lmdb
  - converter
  - parallel_processing
  - threadpoolexecutor
  - batch_processing
  - error_handling
  - missing_data
module: qtaim_gen.source.core.converter
symptoms:
  - "Sequential processing bottleneck on multi-core HPC systems"
  - "Per-item LMDB commits causing ~100x slowdown"
  - "TODO stubs never calling existing parser functions"
  - "Bare except clauses hiding actual errors"
  - "max_readers=1 blocking parallelization"
root_cause: "Incomplete implementation with performance anti-patterns"
commits:
  - 552bbd4
  - 659daa0
  - 6993859
---

# Complete GeneralConverter with Parallel Processing

## Problem Summary

The `GeneralConverter` class was ~90% complete but had critical issues:
1. **Performance**: Sequential processing, per-item LMDB commits, max_readers=1
2. **Incomplete**: TODO stubs for fuzzy/other/bonds data never called existing parsers
3. **Error handling**: 6 bare `except:` clauses hiding errors
4. **Bugs**: Undefined variables, wrong defaults

## Solution Overview

| Fix | Impact |
|-----|--------|
| ThreadPoolExecutor parallelization | ~8x speedup on graph building |
| Batched LMDB commits (500 items) | ~100x faster writes |
| max_readers=1 â†’ 126 | Enables concurrent reads |
| Complete parser integrations | Full data pipeline |
| Specific exception handling | Debuggable errors |

## Implementation Details

### 1. Parallel Processing (3-Phase Approach)

```python
# Phase 1: Sequential grapher initialization with first successful key
self.logger.info("Phase 1: Initializing grapher...")
for idx, key in enumerate(keys_to_iterate):
    # ... initialize grapher with first successful key ...
    break

# Phase 2: Parallel graph building
with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
    futures = {executor.submit(process_key, key): key for key in remaining_keys}

    for future in as_completed(futures):
        key_str, graph, failures = future.result()

        # Phase 3: Sequential scaler updates (not thread-safe)
        if graph is not None:
            self.feature_scaler_iterative.update([graph])
            self.label_scaler_iterative.update([graph])
            write_buffer.append((key_str, serialized_graph))
```

**Why 3 phases?**
- Grapher initialization requires first successful graph (sequential)
- Graph building is independent per molecule (parallel)
- Scaler updates maintain running statistics (sequential)

### 2. Batched LMDB Commits

```python
# Before: Per-item commit (slow)
for key, value in items:
    txn = self.db.begin(write=True)
    txn.put(key, value)
    txn.commit()  # I/O overhead per item

# After: Batched commits
write_buffer = []
for key, value in items:
    write_buffer.append((key, value))
    if len(write_buffer) >= self.batch_size:  # Default: 500
        txn = self.db.begin(write=True)
        for k, v in write_buffer:
            txn.put(k, v)
        txn.commit()
        write_buffer.clear()
```

### 3. Parser Integrations

**Fuzzy data** (`parse_fuzzy_data`):
```python
atom_feats_fuzzy, global_fuzzy_feats = parse_fuzzy_data(
    dict_fuzzy_raw, global_feats["n_atoms"], self.fuzzy_filter
)
```

**Bonds data** (`parse_bond_data`):
```python
bond_feats_from_lmdb, bonds_from_lmdb = parse_bond_data(
    dict_bonds_raw,
    bond_filter=self.bond_filter,
    cutoff=self.bond_cutoff,
    bond_list_definition=self.bond_list_definition,
)
```

**Other data** (`parse_other_data`):
```python
global_other_feats = parse_other_data(dict_other_raw, self.other_filter)
```

### 4. Three Bonding Schemes

```python
if self.bonding_scheme == "qtaim":
    selected_bond_definitions = connected_bond_paths  # QTAIM BCPs
elif self.bonding_scheme == "bonding":
    selected_bond_definitions = bonds_from_lmdb      # From bonds_lmdb
else:  # "structural" (default)
    selected_bond_definitions = bond_list            # RDKit/geometry
```

### 5. Missing Data Strategy

```python
self.missing_data_strategy = config_dict.get("missing_data_strategy", "skip")
self.sentinel_value = config_dict.get("sentinel_value", float("nan"))

# In processing:
if data is None:
    if self.missing_data_strategy == "skip":
        return (key_str, None, failures)  # Skip molecule
    # "sentinel" mode: continue with missing features
```

## New Configuration Options

```json
{
    "n_workers": 8,
    "batch_size": 500,

    "bonding_scheme": "structural",
    "bond_filter": ["fuzzy", "ibsi"],
    "bond_cutoff": 0.3,
    "bond_list_definition": "fuzzy",

    "missing_data_strategy": "skip",
    "sentinel_value": null,

    "fuzzy_filter": null,
    "other_filter": null
}
```

## Files Modified

| File | Changes |
|------|---------|
| `qtaim_gen/source/core/converter.py` | +681/-503 lines |
| `qtaim_gen/source/scripts/helpers/generator_to_embed.py` | +118/-6 lines |

## Prevention Strategies

### Avoid max_readers=1
- Always set `max_readers=126` (LMDB default) for concurrent access
- Add concurrency stress tests to CI

### No Bare Excepts
```python
# Bad
except:
    pass

# Good
except Exception as e:
    self.logger.warning(f"Failed for {key}: {e}")
    failures["type"].append(key)
```

### Complete TODO Stubs
- TODOs should reference GitHub issues
- Stubs should raise `NotImplementedError`, not `pass`

### Batch Database Writes
- Never commit per-item in loops
- Make batch_size configurable

## Verification

```bash
# Run converter with new options
python generator_to_embed.py --config_path ./config.json --converter general

# Check logs for parallel processing
grep "Phase 2: Processing" converter.log
grep "Committed batch" converter.log
```

## Related Documentation

- Brainstorm: `docs/brainstorms/2026-02-02-robust-general-converter-brainstorm.md`
- Plan: `docs/plans/2026-02-02-feat-robust-general-converter-plan.md`
- Tests: `tests/test_lmdb.py`
